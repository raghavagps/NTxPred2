{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab09d86-13e7-46c8-b2b5-f6711b082379",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm2_t30_150M_UR50D and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting initial training...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "NTxPred2 - Training Script with Model Saving and Re-training\n",
    "\n",
    "This script fine-tunes the ESM2-t30_150M model on peptide/protein sequence data \n",
    "to predict neurotoxicity. It performs the following:\n",
    "\n",
    "1. Loads and preprocesses labeled sequence data.\n",
    "2. Trains the ESM2 model on the training data.\n",
    "3. Saves the trained model and tokenizer.\n",
    "4. Reloads the saved model.\n",
    "5. Re-trains the model again using the same dataset.\n",
    "6. Saves the final retrained model.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "from transformers import AutoTokenizer, EsmForSequenceClassification\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.optim import AdamW\n",
    "\n",
    "# -------------------- Configuration --------------------\n",
    "\n",
    "SEED = 42                   # Random seed for reproducibility\n",
    "EPOCHS = 12                 # Number of training epochs\n",
    "BATCH_SIZE = 8              # Training batch size\n",
    "LEARNING_RATE = 1e-5        # Optimizer learning rate\n",
    "MAX_SEQ_LENGTH = 50         # Max tokenized sequence length\n",
    "TRAIN_FILE = \"example_peptide_sequence.csv\"           # Path to training CSV file\n",
    "SAVE_DIR_INITIAL = \"./saved_model_t30\"  # Directory to save first model\n",
    "SAVE_DIR_FINAL = \"./saved_model_t30_final\"  # Directory to save retrained model\n",
    "\n",
    "# -------------------- Reproducibility Setup --------------------\n",
    "\n",
    "def set_seed(seed=SEED):\n",
    "    \"\"\"\n",
    "    Sets the seed for reproducibility across numpy, random, and torch.\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed()\n",
    "\n",
    "# -------------------- Load and Prepare Data --------------------\n",
    "\n",
    "# Load training data from CSV\n",
    "df = pd.read_csv(TRAIN_FILE)\n",
    "\n",
    "# Ensure 'Label' is integer (0 or 1)\n",
    "df['Label'] = df['Label'].astype(int)\n",
    "\n",
    "# Select columns: 'Sequence' and 'Label'\n",
    "df = df.iloc[:, 1:3].reset_index(drop=True)\n",
    "\n",
    "# Extract sequences and labels\n",
    "sequences = df['Sequence'].tolist()\n",
    "labels = torch.tensor(df['Label'].tolist(), dtype=torch.long)\n",
    "\n",
    "# -------------------- Tokenization --------------------\n",
    "\n",
    "# Load pretrained tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/esm2_t30_150M_UR50D\")\n",
    "\n",
    "# Tokenize input sequences (returns padded tensors)\n",
    "tokenized_inputs = tokenizer(\n",
    "    sequences,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=MAX_SEQ_LENGTH,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "# Wrap tokenized inputs and labels into a PyTorch Dataset\n",
    "dataset = TensorDataset(\n",
    "    tokenized_inputs['input_ids'],\n",
    "    tokenized_inputs['attention_mask'],\n",
    "    labels\n",
    ")\n",
    "\n",
    "# Create DataLoader for batch training\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# -------------------- Initialize Model --------------------\n",
    "\n",
    "# Load pretrained model and configure for binary classification\n",
    "model = EsmForSequenceClassification.from_pretrained(\n",
    "    \"facebook/esm2_t30_150M_UR50D\", num_labels=2\n",
    ")\n",
    "\n",
    "# Define optimizer and loss function\n",
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# -------------------- First Training Phase --------------------\n",
    "\n",
    "print(\"\\nüîÅ Starting initial training...\")\n",
    "\n",
    "model.train()\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in dataloader:\n",
    "        input_ids, attention_mask, label_batch = batch\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        loss = loss_fn(logits, label_batch)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS} - Loss: {total_loss:.4f}\")\n",
    "\n",
    "# -------------------- Save Initial Model --------------------\n",
    "\n",
    "os.makedirs(SAVE_DIR_INITIAL, exist_ok=True)\n",
    "model.save_pretrained(SAVE_DIR_INITIAL)\n",
    "tokenizer.save_pretrained(SAVE_DIR_INITIAL)\n",
    "print(f\"\\nüíæ Initial model saved to: {SAVE_DIR_INITIAL}\")\n",
    "\n",
    "# -------------------- Load and Re-initialize Model --------------------\n",
    "\n",
    "print(\"\\nüîÅ Re-loading saved model for re-training...\")\n",
    "\n",
    "# Load the saved model and tokenizer for continued training\n",
    "model = EsmForSequenceClassification.from_pretrained(SAVE_DIR_INITIAL)\n",
    "tokenizer = AutoTokenizer.from_pretrained(SAVE_DIR_INITIAL)\n",
    "\n",
    "# Re-define optimizer for the reloaded model\n",
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# -------------------- Second Training Phase (Re-training) --------------------\n",
    "\n",
    "print(\"\\nüîÅ Starting re-training with the same dataset...\")\n",
    "\n",
    "model.train()\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in dataloader:\n",
    "        input_ids, attention_mask, label_batch = batch\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        loss = loss_fn(logits, label_batch)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Re-train Epoch {epoch+1}/{EPOCHS} - Loss: {total_loss:.4f}\")\n",
    "\n",
    "# -------------------- Save Final Retrained Model --------------------\n",
    "\n",
    "os.makedirs(SAVE_DIR_FINAL, exist_ok=True)\n",
    "model.save_pretrained(SAVE_DIR_FINAL)\n",
    "tokenizer.save_pretrained(SAVE_DIR_FINAL)\n",
    "print(f\"\\n‚úÖ Final retrained model saved to: {SAVE_DIR_FINAL}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84263417-edee-4ae5-9ed2-0f765ae50bec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
