{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22a01ada-d7b7-4f5e-98e2-721a89077921",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n",
      "Extracting embeddings using model: esm2_t30_150M_UR50D, layer: 30\n",
      "Processing batch 1 of 1...\n",
      "Saved embeddings to: esm2_t30_embeddings.csv\n"
     ]
    }
   ],
   "source": [
    "import torch  # PyTorch for model loading and tensor operations\n",
    "import pandas as pd  # For handling dataframes and CSV\n",
    "import pathlib  # For working with file paths\n",
    "from esm import pretrained, FastaBatchedDataset  # ESM utilities for loading model and dataset\n",
    "\n",
    "def extract_esm_embeddings_to_csv(\n",
    "    fasta_file_path,\n",
    "    output_csv_path,\n",
    "    model_name='esm2_t30_150M_UR50D',\n",
    "    repr_layer=30,\n",
    "    tokens_per_batch=4096,\n",
    "    max_seq_len=1500\n",
    "):\n",
    "    \"\"\"\n",
    "    Extract ESM protein embeddings from a FASTA file and save them to a CSV file.\n",
    "\n",
    "    Parameters:\n",
    "    fasta_file_path : str - Path to input FASTA file\n",
    "    output_csv_path : str - Path to output CSV file\n",
    "    model_name : str - ESM model name to use\n",
    "    repr_layer : int - Layer number to extract embeddings from\n",
    "    tokens_per_batch : int - Number of tokens per batch\n",
    "    max_seq_len : int - Maximum length of sequence to consider\n",
    "    \"\"\"\n",
    "\n",
    "    # Load the pretrained ESM model and corresponding alphabet for tokenization\n",
    "    print(\"Loading model...\")\n",
    "    model, alphabet = pretrained.load_model_and_alphabet(model_name)\n",
    "\n",
    "    # Set the model to evaluation mode (no training, disables dropout)\n",
    "    model.eval()\n",
    "\n",
    "    # Move model to GPU if available for faster processing\n",
    "    if torch.cuda.is_available():\n",
    "        model = model.cuda()\n",
    "\n",
    "    # Create a batch converter function to tokenize sequences\n",
    "    batch_converter = alphabet.get_batch_converter(max_seq_len)\n",
    "\n",
    "    # Convert fasta file path to pathlib object\n",
    "    fasta_path = pathlib.Path(fasta_file_path)\n",
    "\n",
    "    # Create a dataset from the input FASTA file\n",
    "    dataset = FastaBatchedDataset.from_file(fasta_path)\n",
    "\n",
    "    # Generate indices to form batches based on token length\n",
    "    batches = dataset.get_batch_indices(tokens_per_batch, extra_toks_per_seq=1)\n",
    "\n",
    "    # Create a DataLoader to iterate through batches efficiently\n",
    "    data_loader = torch.utils.data.DataLoader(\n",
    "        dataset, \n",
    "        collate_fn=batch_converter,  # Convert sequences to model tokens\n",
    "        batch_sampler=batches  # Provide custom batch indices\n",
    "    )\n",
    "\n",
    "    # Initialize lists to collect data for final CSV\n",
    "    sequence_ids = []  # To store sequence headers (SeqID)\n",
    "    sequences = []     # To store raw sequence strings\n",
    "    embeddings = []    # To store embedding vectors\n",
    "\n",
    "    print(f\"Extracting embeddings using model: {model_name}, layer: {repr_layer}\")\n",
    "\n",
    "    # Disable gradient calculations for inference\n",
    "    with torch.no_grad():\n",
    "        # Iterate over each batch\n",
    "        for batch_idx, (labels, seqs, tokens) in enumerate(data_loader):\n",
    "            print(f\"Processing batch {batch_idx + 1} of {len(batches)}...\")\n",
    "\n",
    "            # Move input tokens to GPU if available\n",
    "            if torch.cuda.is_available():\n",
    "                tokens = tokens.cuda(non_blocking=True)\n",
    "\n",
    "            # Perform forward pass to get model outputs\n",
    "            out = model(tokens, repr_layers=[repr_layer], return_contacts=False)\n",
    "\n",
    "            # Extract embeddings from the specified layer\n",
    "            reps = out[\"representations\"][repr_layer].cpu()\n",
    "\n",
    "            # Process each sequence in the current batch\n",
    "            for i, label in enumerate(labels):\n",
    "                entry_id = label.split()[0]  # Extract the sequence ID\n",
    "                sequence = seqs[i]  # Extract the sequence string\n",
    "                seq_len = len(sequence)  # Get length of sequence\n",
    "\n",
    "                # Mean-pool the embedding across sequence length (excluding start/end tokens)\n",
    "                rep = reps[i, 1:seq_len + 1].mean(0).numpy()\n",
    "\n",
    "                # Append ID, sequence, and embedding vector to lists\n",
    "                sequence_ids.append(entry_id)\n",
    "                sequences.append(sequence)\n",
    "                embeddings.append(rep)\n",
    "\n",
    "    # Create a DataFrame from the collected embeddings\n",
    "    df = pd.DataFrame(embeddings)  # Convert list of vectors to DataFrame\n",
    "    df.insert(0, 'Sequence', sequences)  # Add Sequence column at the beginning\n",
    "    df.insert(0, 'SeqID', sequence_ids)  # Add SeqID column before Sequence\n",
    "\n",
    "    # Convert output file path to pathlib object\n",
    "    output_csv_path = pathlib.Path(output_csv_path)\n",
    "\n",
    "    # Save the DataFrame as a CSV file\n",
    "    df.to_csv(output_csv_path, index=False)\n",
    "\n",
    "    # Notify user of successful save\n",
    "    print(f\"Saved embeddings to: {output_csv_path}\")\n",
    "\n",
    "# === Example Usage ===\n",
    "if __name__ == \"__main__\":\n",
    "    extract_esm_embeddings_to_csv(\n",
    "        fasta_file_path=\"example.fasta\",  # Input FASTA\n",
    "        output_csv_path=\"esm2_t30_embeddings.csv\",           # Output CSV\n",
    "        model_name=\"esm2_t30_150M_UR50D\",                    # ESM model\n",
    "        repr_layer=30                                        # Layer to extract\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3a33a7-b24f-4bc9-8757-c2051e1244ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
